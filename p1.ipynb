{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON File\n",
    "csv_root = '''/Users/scl/Python/final/d1/'''\n",
    "json_name = 'filter_all_t.json'\n",
    "with open(csv_root+json_name, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize List to Store Text-Rating Pairs\n",
    "text_rating_pairs = []\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate Over Data in JSON File and Extract Text-Rating Pairs\n",
    "for key, value in data.items():\n",
    "    for element in value:\n",
    "        text = element['review_text']\n",
    "        rating = element['rating']\n",
    "        # Append Text-Rating Tuple to List\n",
    "        text_rating_pairs.append((text, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Length and First 10 Elements of Text-Rating Pairs List\n",
    "print(len(text_rating_pairs))\n",
    "print(text_rating_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New CSV File to Store Text-Rating Pairs\n",
    "csv_file = 'text_rating_pairs.csv'\n",
    "with open(csv_root+csv_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['text', 'rating'])\n",
    "    for text, rating in text_rating_pairs:\n",
    "        writer.writerow([text, rating])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Text Preprocessing Function\n",
    "def text_preprocessing(text):\n",
    "    # Remove Punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove Stop Words\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Convert to Lowercase\n",
    "    text = text.lower()\n",
    "    #  Tokenize Text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Apply Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Apply Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Return Processed Text\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Text Preprocessing Function\n",
    "text = 'This is a great restaurant with delicious food and friendly service. Highly recommended! https://www.google.com'\n",
    "print(text_preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing Function to Apply on DataFrame Rows\n",
    "def preprocess(row):\n",
    "    text = str(row['text'])\n",
    "    text_pp = text_preprocessing(text)\n",
    "    row['text_pp'] = text_pp\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Original CSV and Apply Preprocessing Function\n",
    "csv_df = pd.read_csv(csv_root+csv_file)\n",
    "csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Empty 'text_pp' Column and Apply Preprocessing Function to DataFrame\n",
    "csv_df['text_pp'] = ''\n",
    "pp_df = csv_df.apply(preprocess, axis=1)\n",
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The dataset is split into three parts: training, test, and validation sets, with an 8:1:1 ratio. \n",
    "First, we divide the dataset into a training set (80%) and a test set (20%). \n",
    "Then, we further split the test set into two equal parts to create the test set and validation set (each 10%). \n",
    "It is important to ensure that each rating value is equally distributed across the subsets.\n",
    "'''\n",
    "train_df, test_df = train_test_split(pp_df, test_size=0.2, stratify=pp_df['rating'])\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, stratify=test_df['rating'])\n",
    "\n",
    "# Save Training, Test, and Validation Sets as CSV Files\n",
    "columns_to_save = ['text_pp', 'rating']\n",
    "train_df[columns_to_save].to_csv(csv_root+'train.csv', index=False)\n",
    "test_df[columns_to_save].to_csv(csv_root+'test.csv', index=False)\n",
    "val_df[columns_to_save].to_csv(csv_root+'valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "train_csv = 'train.csv'\n",
    "valid_csv = 'valid.csv'\n",
    "test_csv = 'test.csv'\n",
    "train_df = pd.read_csv(csv_root+train_csv)\n",
    "valid_df = pd.read_csv(csv_root+valid_csv)\n",
    "test_df = pd.read_csv(csv_root+test_csv)\n",
    "print(train_df.head(3))\n",
    "print(valid_df.head(3))\n",
    "print(test_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Dataset in the Required Format for Vectorization and Modeling\n",
    "X_train = train_df[\"text_pp\"].astype(str)\n",
    "X_test = test_df[\"text_pp\"].astype(str)\n",
    "y_train = train_df[\"rating\"].astype(int)\n",
    "y_test = test_df[\"rating\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the TF-IDF Vectorizer to Transform the Text Into a Vector Representation\n",
    "# Set the Maximum Number of Features to 100000, the Minimum Document Frequency to 3, and Use 1-3 Grams\n",
    "vectorizer = TfidfVectorizer(max_features=100000, min_df=3, ngram_range=(1,3))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Logistic Regression Model for Classification Prediction\n",
    "# Set the Regularization Coefficient to 1.0, Use L1 Regularization, \n",
    "# Account for Uneven Label Distribution, and Allow Multiple Iterations for Sparse Data\n",
    "# Training Without Errors or Warnings Indicates Normal Convergence\n",
    "model = LogisticRegression(C=1.0, penalty=\"l1\", class_weight='balanced', max_iter=100, solver='liblinear')\n",
    "model.fit(X_train_vec, y_train)\n",
    "y_pred = model.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model's Performance Using a Confusion Matrix and Calculate Accuracy\n",
    "# Print the Confusion Matrix and Accuracy\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Print the Classification Report for the Model's Performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a Live Test With a Sample Review Text: \"Great Food and Service. Highly Recommended.\"\n",
    "new_text = \"Great food and service. Highly recommended.\"\n",
    "new_text_pp = text_preprocessing(new_text)\n",
    "print(new_text_pp)\n",
    "\n",
    "# Transform the Preprocessed Text Into a TF-IDF Vector\n",
    "new_vector = vectorizer.transform([new_text_pp])\n",
    "print(new_vector)\n",
    "\n",
    "# Predict the Rating for the New Text\n",
    "new_pred = model.predict(new_vector)\n",
    "print(new_pred)\n",
    "\n",
    "# Print the Prediction Result\n",
    "print(\"The predicted rating for the new text is:\", new_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will analyze the weight-feature word relationship.\n",
    "# Examine the model's weights, which form a matrix with a shape of (number of categories * number of features)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second feature appears to have a significant downward trend; \n",
    "# Let's determine which feature corresponds to it\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(feature_names[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all indices with significant weights\n",
    "indices = np.where(np.all(np.diff(model.coef_, axis=0) < 0, axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the names of these features\n",
    "selected_feature_names = np.take(feature_names, indices)\n",
    "selected_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud Analysis\n",
    "train_csv = 'train.csv'\n",
    "df = pd.read_csv(csv_root+train_csv)\n",
    "\n",
    "# Filter out reviews with ratings of 1 and 4-5\n",
    "low_ratings = df[(df['rating'] <= 1)]\n",
    "high_ratings = df[(df['rating'] >= 4) & (df['rating'] <= 5)]\n",
    "\n",
    "# Merge the review text into one string\n",
    "low_text = ' '.join(low_ratings['text_pp'].astype(str))\n",
    "high_text = ' '.join(high_ratings['text_pp'].astype(str))\n",
    "\n",
    "# Generate Word Clouds\n",
    "low_wc = WordCloud().generate(low_text)\n",
    "high_wc = WordCloud().generate(high_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the word cloud for negative reviews\n",
    "plt.imshow(low_wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the word cloud for positive reviews\n",
    "plt.imshow(high_wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
