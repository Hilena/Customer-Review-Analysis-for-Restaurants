{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hilinasisay/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON File\n",
    "csv_root = '''/Users/scl/Python/final/d1/'''\n",
    "json_name = 'image_review_all.json'\n",
    "list_of_dict = []\n",
    "for line in open(csv_root+json_name, 'r'):\n",
    "    list_of_dict.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a List to Store the Relationship Between Text Evaluations and Ratings, Including User ID and Restaurant ID\n",
    "final_res_list = []\n",
    "#list_of_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize User and Restaurant Index Counters\n",
    "cur_user_index = 0\n",
    "cur_rest_index = 0\n",
    "\n",
    "# Create an Empty List to Store the Final Results\n",
    "user_map = {}\n",
    "rest_map = {}\n",
    "\n",
    "# Define Functions to Handle User and Restaurant IDs, and Process the Data From the JSON File\n",
    "def handle_user_id(user_id_str):\n",
    "    if user_id_str in user_map:\n",
    "        return user_map[user_id_str]\n",
    "    global cur_user_index\n",
    "    user_map[user_id_str] = cur_user_index\n",
    "    cur_user_index += 1\n",
    "    return user_map[user_id_str]\n",
    "\n",
    "def handle_rest_id(rest_id_str):\n",
    "    if rest_id_str in rest_map:\n",
    "        return rest_map[rest_id_str]\n",
    "    global cur_rest_index\n",
    "    rest_map[rest_id_str] = cur_rest_index\n",
    "    cur_rest_index += 1\n",
    "    return rest_map[rest_id_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate Through Each Element in the List of Dictionaries From the JSON File\n",
    "for element in list_of_dict:\n",
    "    # Extract User and Restaurant IDs\n",
    "    user_id_str = element['user_id']\n",
    "    rest_id_str = element['business_id']\n",
    "    # Process User and Restaurant IDs Using the Defined Functions\n",
    "    user_id = handle_user_id(user_id_str)\n",
    "    rest_id = handle_rest_id(rest_id_str)\n",
    "    # Extract the Review Text and Rating\n",
    "    text = element['review_text']\n",
    "    rating = element['rating']\n",
    "    # Create a Tuple Containing User ID, Review Text, Restaurant ID, and Rating\n",
    "    # Add the Tuple to the Final Results List\n",
    "    final_res_list.append((user_id, text, rest_id, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Length and First 10 Elements of Text-Rating Pairs List\n",
    "print(len(final_res_list))\n",
    "print(final_res_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New CSV File to Store Text-Rating Pairs\n",
    "csv_file = 'text_rating_pairs_huge_with_id.csv'\n",
    "with open(csv_root+csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['user_id','text','rest_id', 'rating'])\n",
    "    for user_id, text, rest_id, rating in final_res_list:\n",
    "        writer.writerow([user_id, text, rest_id, rating])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Text Preprocessing Function\n",
    "def text_preprocessing(text):\n",
    "    # Remove Punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove Stop Words\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Convert to Lowercase\n",
    "    text = text.lower()\n",
    "    #  Tokenize Text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Apply Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Apply Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Return Processed Text\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Text Preprocessing Function\n",
    "text = 'This is a great restaurant with delicious food and friendly service. Highly recommended! https://www.google.com'\n",
    "print(text_preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing Function to Apply on DataFrame Rows\n",
    "def preprocess(row):\n",
    "    text = str(row['text'])\n",
    "    text_pp = text_preprocessing(text)\n",
    "    row['text_pp'] = text_pp\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Original CSV and Apply Preprocessing Function\n",
    "csv_df = pd.read_csv(csv_root+csv_file)\n",
    "csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Empty 'text_pp' Column and Apply Preprocessing Function to DataFrame\n",
    "csv_df['text_pp'] = ''\n",
    "pp_df = csv_df.apply(preprocess, axis=1)\n",
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The dataset is split into three parts: training, test, and validation sets, with an 8:1:1 ratio. \n",
    "First, we divide the dataset into a training set (80%) and a test set (20%). \n",
    "Then, we further split the test set into two equal parts to create the test set and validation set (each 10%). \n",
    "It is important to ensure that each rating value is equally distributed across the subsets.\n",
    "'''\n",
    "train_df, test_df = train_test_split(pp_df, test_size=0.2, stratify=pp_df['rating'])\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, stratify=test_df['rating'])\n",
    "\n",
    "# Save Training, Test, and Validation Sets as CSV Files\n",
    "columns_to_save = ['user_id', 'text_pp', 'rest_id', 'rating']\n",
    "train_df[columns_to_save].to_csv(csv_root+'train_huge_with_id.csv', index=False)\n",
    "test_df[columns_to_save].to_csv(csv_root+'test_huge_with_id.csv', index=False)\n",
    "val_df[columns_to_save].to_csv(csv_root+'valid_huge_with_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "train_csv = 'train_huge_with_id.csv'\n",
    "valid_csv = 'valid_huge_with_id.csv'\n",
    "test_csv = 'test_huge_with_id.csv'\n",
    "train_df = pd.read_csv(csv_root+train_csv)\n",
    "valid_df = pd.read_csv(csv_root+valid_csv)\n",
    "test_df = pd.read_csv(csv_root+test_csv)\n",
    "print(train_df.head(3))\n",
    "print(valid_df.head(3))\n",
    "print(test_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df, valid_df, test_df])\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a OneHotEncoder for processing User IDs\n",
    "one_hot_user = OneHotEncoder(handle_unknown='ignore')\n",
    "# Define a OneHotEncoder for processing Restaurant IDs\n",
    "one_hot_rest = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the TF-IDF Vectorizer to Transform the Text Into a Vector Representation\n",
    "# Set the Maximum Number of Features to 100000, the Minimum Document Frequency to 3, and Use 1-3 Grams\n",
    "vectorizer = TfidfVectorizer(max_features=100000, min_df=3, ngram_range=(1,3))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessor that combines the three encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('user', one_hot_user, ['user_id']),\n",
    "        ('rest', one_hot_rest, ['rest_id']),\n",
    "        ('text', vectorizer, 'text_pp')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logistic regression model\n",
    "lr = LogisticRegression(C=1.0, penalty=\"l1\", class_weight='balanced', max_iter=100, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Overall Model by Connecting the Preprocessor and Logistic Regression Using a Pipeline\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),('classifier', lr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_df['text_pp'] = train_df['text_pp'].fillna('')\n",
    "model.fit(train_df[['user_id', 'rest_id', 'text_pp']], train_df['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_df['text_pp'] = test_df['text_pp'].fillna('')\n",
    "# Predict Ratings\n",
    "y_pred = model.predict(test_df[['user_id', 'rest_id', 'text_pp']])\n",
    "# Calculate the Accuracy\n",
    "accuracy = accuracy_score(test_df['rating'], y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model's Performance Using a Confusion Matrix and Calculate Accuracy\n",
    "# Print the Confusion Matrix and Accuracy\n",
    "y_test = test_df['rating']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Print the Classification Report for the Model's Performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The final accuracy has increased by one percentage point. \n",
    "The f1 scores corresponding to all rating categories, except for the rating of 2, have increased as well. \n",
    "The experimental results show that incorporating user and restaurant information has improved the model's performance, \n",
    "making the entire system more accurate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a Live Test With a Sample Review Text: \"Great Food and Service. Highly Recommended.\"\n",
    "new_text = \"Great food and service. Highly recommended.\"\n",
    "new_text_pp = text_preprocessing(new_text)\n",
    "print(new_text_pp)\n",
    "\n",
    "# Transform the Preprocessed Text Into a TF-IDF Vector\n",
    "new_vector = vectorizer.transform([new_text_pp])\n",
    "print(new_vector)\n",
    "\n",
    "# Predict the Rating for the New Text\n",
    "new_pred = model.predict(new_vector)\n",
    "print(new_pred)\n",
    "\n",
    "# Print the Prediction Result\n",
    "print(\"The predicted rating for the new text is:\", new_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will analyze the weight-feature word relationship.\n",
    "# Examine the model's weights, which form a matrix with a shape of (number of categories * number of features)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second feature appears to have a significant downward trend; \n",
    "# Let's determine which feature corresponds to it\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(feature_names[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all indices with significant weights\n",
    "indices = np.where(np.all(np.diff(model.coef_, axis=0) < 0, axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the names of these features\n",
    "selected_feature_names = np.take(feature_names, indices)\n",
    "selected_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We can see that some of these words have strong emotional connotations\n",
    "For instance, undercooked, tasteless, worst, cold, frozen, and burned are very negative comments about a restaurant.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud Analysis\n",
    "train_csv = 'train.csv'\n",
    "df = pd.read_csv(csv_root+train_csv)\n",
    "\n",
    "# Filter out reviews with ratings of 1 and 4-5\n",
    "low_ratings = df[(df['rating'] <= 1)]\n",
    "high_ratings = df[(df['rating'] >= 4) & (df['rating'] <= 5)]\n",
    "\n",
    "# Merge the review text into one string\n",
    "low_text = ' '.join(low_ratings['text_pp'].astype(str))\n",
    "high_text = ' '.join(high_ratings['text_pp'].astype(str))\n",
    "\n",
    "# Generate Word Clouds\n",
    "low_wc = WordCloud().generate(low_text)\n",
    "high_wc = WordCloud().generate(high_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the word cloud for negative reviews\n",
    "'''\n",
    "We can see that there are many negative comments about the burger.\n",
    "People often complain that they ordered something but got another thing, expressing dissatisfaction with the result.\n",
    "'''\n",
    "plt.imshow(low_wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the word cloud for positive reviews\n",
    "plt.imshow(high_wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
